---
title: "STAT3040 Final Project"
author: 
- "Alvis Chan"
- "u7287079"
date: '2023-06-5'
output:
  pdf_document: default
  html_document: default

header-includes:
  - \usepackage{wrapfig}
  - \usepackage{lipsum}
  - \usepackage{geometry}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \usepackage{mwe}
---

<!-- Finalized code blocks indicated with {r name_Fin} -->
<!-- Runtime is long due to SVM tuning and NN Modelling -->

```{r setup_def_Fin, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup2_libs_Fin, include=FALSE}
library(knitr) #citation
library(caTools) #citation
library(tidyverse) #citation
library(psych) #citation
library(corrplot) #citation
library(GGally) #citation
library(ggplot2) #citation
library(gridExtra) #citation
library(MASS) #citation
library(dplyr) #citation
library(stats) # R core team citation
library(car) #citation
library(caret)#citation
library(grid) # R core team citation
library(kableExtra) #citation
library(naniar) #citation
library(mice) #citation
library(leaps) #citation
library(glmnet) #citation
library(mgcv) #citation
library(splines) #R core team citation
library(gam) #citation
library(tree) #citation
library(randomForest) #citation
library(gbm) #citation
library(nnet) #citation
library(keras) #citation
library(tensorflow) #citation
library(naivebayes) #citation
library(class) #citation
library(pROC) #citation
library(e1071) #citation
```

```{r setup3_Fin, include=FALSE}
setwd('C:/DEV/STAT3001/Ass3')
set.seed(211)

train <- read.csv("train.csv", header=T)
test <- read.csv("test.csv", header=T)
```

```{r output, eval = FALSE, include = FALSE}
# change the model, get the pred, pass to output (for price_sR), get csv
pred <- predict(tre.bag,newdata = ntest)
output <- pred^2 * sign(pred)
f_output <- cbind(test$id,output)
write.csv(f_output, file = "submition.csv", row.names = FALSE)
```
## Introduction

This project has two core objectives:

- Training a model to predict the price of advertised housing
- Training a model to classify housing close to schools.

There are two datasets available for the project:

- Training dataset: Consisting of 5000 samples of 13 covariates with response variables *price* and *school*
- Testing dataset: Consisting of 3000 samples of 13 covariates without response variables

For each objective, 5 mainstream modelling classes are considered and their respective 
strengths, limitations and performances are analyzed and discussed. The algorithms 
and classes included are:

- Prediction
  + Linear Regression
  + Lasso Regularization
  + Polynomial, Splines and General Additive Models
  + Regression Decision Trees
  + Neural Networks
- Classification
  + Logistic Regression
  + Discriminant Analysis
  + Classification Decision Trees
  + Support Vector Machines
  + Neural Networks

Additionally, the analysis is evaluated alongside benchmark metrics obtained through 
simpler methods, these naive or simple methods will be breifly covered at the start of 
each major section:

- Additional classes
  + Mean/Median Prediction
  + Naive Bayes Classifier
  + K-Nearest-Neighbours

\newpage
## Explaratory Data Analytics

While examining the data, we observe occurrence of missing data (Table 1)

```{r checkMissing_Fin, echo = FALSE, out.length = "70%"}
Missing <- paste0((round(colSums(is.na(train)) / nrow(train),3) * 100),'%')
Missing_1 <- data.frame(Missing[1:8]) %>% t()
Missing_2 <- data.frame(Missing[9:16]) %>% t()

colnames(Missing_1) <- c('id','price','school','lat','lon','rate','hoa','hvac')
colnames(Missing_2) <- c('garage','view','year','bath','bed','stories','lot','living')
rownames(Missing_1) <- 'Missing (%)'
rownames(Missing_2) <- 'Missing (%)'

kable(Missing_1, align = c("c"), caption = "Percentage of Missing Data") %>%
  kable_styling(latex_options = "HOLD_position")

kable(Missing_2, align = c("c")) %>%
  kable_styling(latex_options = "HOLD_position")
```

```{r mcar_test_Fin, echo = FALSE, out.length = "70%"}
temp <- mcar_test(train)
temp <- data.frame(round(temp,0))
```
\begin{wraptable}{r}{6cm}
    \begin{tabular}{c|c}
     \textbf{} & \textbf{MCAR Results}\\
      \hline
      Statistic & `r temp$statistic`\\
      df & `r temp$df`\\
      p.value & `r temp$p.value`\\
      missing patterns & `r temp$missing.patterns`\\
    \end{tabular}
 \end{wraptable}
 
Conduct a missing completely at random (MCAR) test. From results, data is not 
missing at random (MNAR). as such we cannot omit the missing values and have to 
implement data imputation.

While there are specialized methods to deal with MNAR data, such as selection models
 and pattern mixture model$^{1}$. The underlying logic and knowledge is beyond 
 the scope of the project. Therefore, we use multiple imputation in attempt to partially 
 eliminate bias of MNAR data with the *mice* package$^{2}$.
 
 The parameters used are 7 imputations using a defined pattern-mixture modelling 
 method  in *mice* package.
 
```{r fill_missing1_Fin, cache = TRUE, echo = TRUE, results = "hide", warning = FALSE}
# Imputation on missing data
impute <- mice(train, m = 7, method = c("pmm"), seed = 211)
```
```{r fill_missing2_Fin, echo = FALSE}
# Access the completed imputed data
data <- complete(impute)
```
```{r testing_setup, include = FALSE}
t_impute <- mice(test, m = 7, method = c("pmm"), seed = 211)
ntest <- complete(t_impute)

ntest$rate <- abs(ntest$rate)^(1/2) * sign(ntest$rate)
ntest$living <- abs(ntest$living)^(1/2) * sign(ntest$living)
ntest$lot <- abs(ntest$lot)^(1/2) * sign(ntest$lot)
colnames(ntest)[colnames(ntest) == "rate"] <- "rate_sR"
colnames(ntest)[colnames(ntest) == "living"] <- "living_sR"
colnames(ntest)[colnames(ntest) == "lot"] <- "lot_sR"
```

```{R checkSummary1_Fin, echo = FALSE, out.width = "30%", out.length = "30%"}
#describe from psych package
con_cov <- c("price","lat","lon","rate","garage","year","bath","bed","stories","lot","living")
summary <- describe(data[which(colnames(data) %in% con_cov)]) %>% 
  select_if(is.numeric) %>% t() %>% 
  as.data.frame() %>%
  mutate_if(is.numeric, round, 1)
summary <- summary %>% 
  kable(align = "c", caption = "Summary Statistics for Continuous Variables in Train Dataset") %>%
  kable_styling(latex_options = "HOLD_position")

summary
```

The assumption of normality is crucial in ensuring the validity of statistical 
inference of tests and models performed on the data later in the report.
Kurtosis identifies the shape of the distribution and skew measures asymmetry of 
the distribution. Acceptable ranges of skew and Kurtosis values are within [-3,3] and 
[-10,10] respectively$^3$.

At first glance of the summary table (Table 2), the skew and kurtosis values of 
*lat*, *lon*, *year*, *bath*, *bed*, *stories*, *garage* are within appropriate 
range while *price*, *rate*, *lot* and *living* are out of range.

Variables within the range display similar means and medians, suggesting a symmetrical
distribution and adherence to the assumption of normality.

However, for other variables such as price and lot, notably high kurtosis values
of 193.8 and 455.4, along with skewness values of 9.9 and 12.5, respectively,
indicate a pronounced presence of outliers.

```{R checkSummary2_Fin, echo = FALSE, out.width = "30%", out.length = "30%"}
data_sim <- data[!(names(data) %in% c("id", "school","hoa","hvac","view","lat","lon","year",
                                      "bath","bed","stories","garage"))]

data_sim$price_sR <- abs(data_sim$price)^(1/2) * sign(data_sim$price)
data_sim$rate_sR <- abs(data_sim$rate)^(1/2) * sign(data_sim$rate)
data_sim$living_sR <- abs(data_sim$living)^(1/2) * sign(data_sim$living)
data_sim$lot_sR <- abs(data_sim$lot)^(1/2) * sign(data_sim$lot)

summary <- describe(data_sim[which(colnames(data_sim) %in% c("price_sR","rate_sR","lot_sR","living_sR"))]) %>% 
  select_if(is.numeric) %>% t() %>% 
  as.data.frame() %>%
  mutate_if(is.numeric, round, 1)
```

\begin{wraptable}{r}{6cm}
    \caption{Cube Root Transformed Statistics}
    \begin{tabular}{c|c|c|c|c}
     \textbf{} & \textbf{price} & \textbf{rate} & \textbf{living} & \textbf{lot}\\
      \hline
      mean & `r summary$price_sR[3]` & `r summary$rate_sR[3]` & `r summary$lot_sR[3]` & `r summary$living_sR[3]`\\
      median & `r summary$price_sR[4]` & `r summary$rate_sR[4]` & `r summary$lot_sR[4]` & `r summary$living_sR[4]`\\
      min & `r summary$price_sR[8]` & `r summary$rate_sR[8]` & `r summary$lot_sR[8]` & `r summary$living_sR[8]`\\
      max & `r summary$price_sR[9]` & `r summary$rate_sR[9]` & `r summary$lot_sR[9]` & `r summary$living_sR[9]`\\
      range & `r summary$price_sR[10]` & `r summary$rate_sR[10]` & `r summary$lot_sR[10]` & `r summary$living_sR[10]`\\
      skew & `r summary$price_sR[11]` & `r summary$rate_sR[11]` & `r summary$lot_sR[11]` & `r summary$living_sR[11]`\\
      kutosis & `r summary$price_sR[12]` & `r summary$rate_sR[12]` & `r summary$lot_sR[12]` & `r summary$living_sR[12]`\\
    \end{tabular}
 \end{wraptable}

Therefore, to meet the normality assumption, transformation of these variables
will be necessary. Due to presence of negative values, absolute square root transformations
are applied onto the variables.

$$x_{new} = \sqrt{|x|} \times sgn(x) $$

The summary statistics in Table 3 indicate an improvement in the distribution of 
variables after applying a cube root transformation. While the kurtosis and skew 
for *rate* remains outside acceptable range after transformation, implying that 
the variable is prone to outliers. Considering the mean and median of *rate* are 
both negative, conventional transformations such as log and roots are limited. As 
such we will leave *rate* as a caveat in this project.

For categorical variables (Table 3.1), the distribution of *school* and *hoa* are relatively 
balanced while *hvac* and *view* are significantly unbalanced (17:83 and 78:22 
respectively). This may introduce bias towards majority classes when training models.
To address this issue, oversampling and undersampling techniques are incooperated into
resampling methods to create a more balanced dataset for model training.

```{r refresh1_FIN, echo = FALSE}
data$price <- abs(data$price)^(1/2) * sign(data$price)
data$rate <- abs(data$rate)^(1/2) * sign(data$rate)
data$living <- abs(data$living)^(1/2) * sign(data$living)
data$lot <- abs(data$lot)^(1/2) * sign(data$lot)

colnames(data)[colnames(data) == "price"] <- "price_sR"
colnames(data)[colnames(data) == "rate"] <- "rate_sR"
colnames(data)[colnames(data) == "living"] <- "living_sR"
colnames(data)[colnames(data) == "lot"] <- "lot_sR"
```

```{r refresh2, echo = FALSE}
set.seed(659)
lm.one <- data[which(data$hvac == 0 & data$view == 0),]
lm.two <- data[which(data$hvac != 0 & data$view == 0),]
lm.three <- data[which(data$hvac == 0 & data$view != 0),]
lm.four <- data[which(data$hvac != 0 & data$view != 0),]

indices <- sample(nrow(lm.one))
fold1 <- data[indices[1:round(0.8*nrow(lm.one))], ]
val1 <- data[indices[(round(0.8*nrow(lm.one))+1):nrow(lm.one)], ]
indices <- sample(nrow(lm.two))
fold2 <- data[indices[1:round(0.8*nrow(lm.two))], ]
val2 <- data[indices[(round(0.8*nrow(lm.two))+1):nrow(lm.two)], ]
indices <- sample(nrow(lm.three))
fold3 <- data[indices[1:round(0.8*nrow(lm.three))], ]
val3 <- data[indices[(round(0.8*nrow(lm.three))+1):nrow(lm.three)], ]
indices <- sample(nrow(lm.four))
fold4 <- data[indices[1:round(0.8*nrow(lm.four))], ]
val4 <- data[indices[(round(0.8*nrow(lm.four))+1):nrow(lm.four)], ]

#og <- data
val <- rbind(val1,val2,val3,val4)
data <- rbind(fold1,fold2,fold3,fold4)
```

```{r cat_var1_Fin, echo = FALSE, fig.height = 2.2, fig.width= 6}
par(mfrow=c(1,4))
barplot(table(data$school), main = "school")
barplot(table(data$hoa), main = "hoa")
barplot(table(data$hvac), main = "hvac")
barplot(table(data$view), main = "view")
title(main = "Table 3.1: Distriution of Binary Variables", line = -1, outer = TRUE)
```

```{r cor_1_FIN, echo = FALSE}
temp <- round(cor(data[2:16],method="pearson"),2)

kable(temp, align = c("c"), caption = "Pearson Correlation") %>%
  kable_styling(latex_options = c("striped", "scale_down", "HOLD_position"))
```

The response variables, *price* and *school*, do not exhibit strong correlation 
with majority variables (Table 4). Notably, *price* and *living_sR* are the only pair with 
moderate correlation (0.61) while the highest correlated variable with *school* 
is *year* (0.33). Pearson correlation captures linear relationships but does not 
assume non-linear relationships accurately. Therefore, it is possible the covariates 
are not linear with responses.

Collinearity between covariates influences the reliability and interpretability of 
the trained models. In Table 4, some moderate correlations is discovered with *bath* 
and *living_SR* being most linearly correlated at 0.77. To mitigate impacts of 
multicollinearity, covariate selection and regularization techniques such as foward-selection 
and ridge regression are applied when modelling the data.

## Naive Predictions

To predict *price_sR* with the provided trainin datam the simplest prediction 
technique would be to use mean or median as response for all observations. 
Using mean squared error (MSE) as performance criterion, a 10-fold cross validation 
is conducted and the average MSE is used as baseline for more complex analysis.

```{r np_baseline_Fin, echo = FALSE, cache = TRUE}
n <- nrow(data) #no. of data
K <- 10 #cv count
errs <- rep(NA,K)
errs2 <- rep(NA,K)

fold <- sample(rep(1:K, each=n/K))

for(k in 1:K) {
  df.train <- data[fold != k,]
  df.test <- data[fold == k,]
  
  pred1 <- mean(df.train$price_sR)
  pred2 <- median(df.train$price_sR)
  
  errs[k] <- mean((df.test$price_sR-pred1)^2) 
  errs2[k] <- mean((df.test$price_sR-pred2)^2) 
}
```
Using R, the MSE for mean approach is `r round(mean(errs),3)` and the median approach is
`r round(mean(errs2),3)`.

```{r pred_setup, echo = FALSE}
pr <- data[which(!colnames(data) %in% c("id",'school'))]
```
## Prediction 1: Linear Model
<!-- final mod in variable lm.mod-->
A linear model assumes linear relationship between covariates and its response. 
To optimize the results, a best subset selection is performed and the selected 
subsets are passed to a custom algorithm for 10-fold cross-validation (10CV) 
to finalize the best performing subset and model coefficients for the linear model.
 
```{r lm_cv1_Fin, cache = TRUE, echo = FALSE, fig.height = 4, fig.width = 10}
set.seed(83)

lm.one <- pr[which(pr$hvac == 0 & pr$view == 0),]
lm.two <- pr[which(pr$hvac != 0 & pr$view == 0),]
lm.three <- pr[which(pr$hvac == 0 & pr$view != 0),]
lm.four <- pr[which(pr$hvac != 0 & pr$view != 0),]

K <- 10

n <- nrow(lm.one)
fold1 <- sample(rep(1:K, each=n/K))
n <- nrow(lm.two)
fold2 <- sample(rep(1:K, each=n/K))
n <- nrow(lm.three)
fold3 <- sample(rep(1:K, each=n/K))
n <- nrow(lm.four)
fold4 <- sample(rep(1:K, each=n/K))

cv.mse <- matrix(NA,K,13)
val.mse <- rep(1,K)

for(k in 1:K) {
  df.train <- rbind(lm.one[fold1 != k,],lm.two[fold2 != k,],lm.three[fold3 != k,],lm.four[fold4 != k,])
  df.test <- rbind(lm.one[fold1 == k,],lm.two[fold2 == k,],lm.three[fold3 == k,],lm.four[fold4 == k,])
  
  X <- as.matrix(df.train[which(!colnames(df.train) %in% c("price_sR"))])
  Y <- as.matrix(df.train$price_sR)
  sm <- summary(regsubsets(X, Y, nvmax= 13))
  
  #find best performing subset
  for(i in 1:13) {
    temp <- sm$outmat[i,]
    f <- paste(names(temp[which(temp=='*')]),collapse = " + ")
    f <- paste('price_sR ~', f)
    
    cv_mod <- glm(as.formula(f), data = df.train)
    
    pred <- predict(cv_mod, newdata = df.test)
    cv.mse[k,i] <- mean((pred-df.test$price_sR)^2)
    
    if(i==5) {
      cv.mod <- cv_mod
    }
  }
  
  pred <- predict(cv.mod, newdata = val)
  mse <- mean((pred-val$price_sR)^2)
  val.mse[k] <- mse
  if(min(val.mse)==mse) {
    f.mse <- mse
    lm.mod <- cv.mod
    cv.f.mod <- summary(cv.mod)$coefficients
  }

}

par(mfrow=c(1,2))
plot(1:13,cv.mse[1,], type = "b", lwd = 2, ylab = "CV MSE", xlab = 'Number of variables in Subset', ylim = c(0.15,0.35))
for(i in 2:K){ 
  lines(cv.mse[i,], type="b", lwd=2, col=i) 
} #3 performs best


cv.est <- apply(cv.mse, 2, mean)
cv.sd <- apply(cv.mse, 2, sd)/sqrt(K)

plot(1:13, cv.est, lwd=3, type="b", ylim=c(0.20, 0.33),
ylab="CV MSE Estimate", xlab="Number of variables in Subset")
points(1:13, cv.est + cv.sd, pch=25, col="red")
points(1:13, cv.est - cv.sd, pch=24, col="red")

#real <- predct(cv.mod)
title(main = "Figure 1: Cross-validation MSE and Estimates on Subset Selection", line = -1, outer = TRUE)
```

\begin{wraptable}{r}{6cm}
    \caption{Final GLM Model}
    \begin{tabular}{c|c|c}
     \textbf{Covariate} & \textbf{Estimate} & \textbf{p-value}\\
      \hline
      (Intercept) & `r round(cv.f.mod[1,1],3)` & `r round(cv.f.mod[1,4],3)`\\
      rate\_sR & `r round(cv.f.mod[2,1],3)` & `r round(cv.f.mod[2,4],3)`\\
      hoa & `r round(cv.f.mod[3,1],3)` & `r round(cv.f.mod[3,4],3)`\\
      year & `r round(cv.f.mod[4,1],3)` & `r round(cv.f.mod[4,4],3)`\\
      bath & `r round(cv.f.mod[5,1],3)` & `r round(cv.f.mod[5,4],3)`\\
      living\_sR & `r round(cv.f.mod[6,1],3)` & `r round(cv.f.mod[6,4],3)`\\
    \end{tabular}
 \end{wraptable}
 
The curve plateaus at around the 5-covariate subset (Figure 1), implying the linear model 
exhibits best performance at that range.as it plateus. The confidence intervals 
provides similar observations with 5-covariate subset achieving the lowest intervals.

The final model for multiple linear regression through cross validation and **average**
**test MSE is `r round(mean(val.mse),3)`.** The best subset has a similar testing MSE 
of `r round(f.mse,3)`. A clear interpretation of the model can be made using 
$$ \hat{y_{priceSR}} = \hat{\beta_0} - \hat{\beta_1}\cdot rate\_sr - \hat{\beta_2}\cdot hoa - \hat{\beta_3}\cdot year + \hat{\beta_4}\cdot bath + \hat{\beta_5}\cdot living$$
The model confirms some common intuitions, such as a positive correlation between 
living room size and price, and the negative correlation between tax rates and price. 
Additionally, the number of baths and novelty of housing correlates with price.

\newpage
\restoregeometry

## Prediction 2:  Lasso regression

Lasso is a regularization technique applied on linear regression. The objective of 
linear models covered in the prior section is to minimize the sum of residuals. 
In Lasso regression, an additional term is added on the objective function, which 
is the sum of the absolute values of the coefficients multiplied by a tuning 
parameter lambda. Therefore minimizing the quantity$^4$
$$ \sum_{i=1}^n\Big(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\Big)^2+\lambda\sum_{j=1}^p|\beta_j| $$

Similar to LM we will fit a linear model using 10-fold cross validation with lasso 
regularization with *glmnet* package.

```{r las_cv1, cache=TRUE, echo = FALSE, fig.height = 3.5, fig.width = 9}
set.seed(83)

X <- as.matrix(pr[ , !colnames(pr) %in% "price_sR"])
Y <- pr$price_sR

valx <- as.matrix(val[ , !colnames(val) %in% c("price_sR","school","id")])
valy <- val$price_sR

las_mod <- glmnet(X,Y,alpha=1,standardize=FALSE)

par(mfrow=c(1,2))
plot(las_mod, xvar="lambda")
title(main = 'Figure 2: Covariate Coefficients and Lambda', line = 3)

las_mod <- cv.glmnet(X, Y, alpha = 1, standardize = FALSE)
lambda <- log(las_mod$lambda.min)
pred <- predict(las_mod, newx = valx, s = lambda)
mse <- round(mean((pred - valy)^2),4)
cofs <- coef(las_mod)

plot(las_mod)
title(main = 'Figure 3: MSE and Lambda', line = 3)

abline(v=lambda, col="cyan3", lwd=3)
abline(h=las_mod$cvm[which.min(las_mod$cvm)], lwd=3,
col="dark orange")
```

\begin{wraptable}{r}{4cm}
    \caption{Final Lasso Model}
    \begin{tabular}{c|c}
     \textbf{Covariate} & \textbf{Estimate}\\
      \hline
      (Intercept) & `r round(cofs[1,],3)`\\
      lat & `r round(cofs[2,],3)`\\
      lon & `r round(cofs[3,],3)`\\
      rate\_sR & `r round(cofs[4,],3)`\\
      hoa & `r round(cofs[5,],3)`\\
      hvac & `r round(cofs[6,],3)`\\
      garage & `r round(cofs[7,],3)`\\
      view & `r round(cofs[8,],3)`\\
      year & `r round(cofs[9,],3)`\\
      bath & `r round(cofs[10,],3)`\\
      bed & `r round(cofs[11,],3)`\\
      stories & `r round(cofs[12,],3)`\\
      lot\_sR & `r round(cofs[13,],3)`\\
      living\_sR & `r round(cofs[14,],3)`\\
    \end{tabular}
\end{wraptable}

Using a lambda of `r round(lambda,3)` (Figure 3), the final complied model 
consists of 8 parameters and a testing MSE of `r mse`, which a slight decline to 
linear regression (Figure 4). However, the interpretation of predictors and response 
remains similar with linear regression, which reinforces the relationships determined 
in the prior section.

```{r pred1_Fin, echo = FALSE, fig.width = 8, fig.height = 4}
par(mfrow=c(1,2))
pred_int <- predict(lm.mod, newdata = val)

# Create a scatter plot of actual vs. predicted values
plot(pred_int, val$price_sR, xlab = "Actual Values", ylab = "Predicted Values", main = "Linear Tuned")

abline(a=-0.1, b=0.868, col = "red",lty=2)

# Create a scatter plot of actual vs. predicted values
plot(pred, val$price_sR, xlab = "Actual Values", ylab = "Predicted Values", main = "Lasso Regularized")

abline(a=-0.1, b=0.868, col = "red",lty=2)

title(main='Figure 4: Actual vs Predicted Values',line = -1, outer=TRUE)
```

Lasso regularization excels in covariate selection, flexibility and being robust 
with multicollinearity. However, Lasso also underestimates coefficients 
when the sample size is small, hence influencing its model predictive capability.

\newpage
\restoregeometry

## Prediction 3: General Additive Models

In Predictions 1 and 2 linearity between covariates is assumed. For this section, 
potential non-linear relationships between the variables are considered. We use 
polynomial regression to examine potential covariates sharing non-linear relationships 
with *price_sR* and apply cubic splines to train generalized additive models.

```{r poly_setup_Fin, cache = TRUE, echo = FALSE, fig.height = 3, fig.width = 10}
degrees <- 1:10
polyy <- function(variable) {
  train_mse <- sapply(degrees, function(degree) {
    formula <- as.formula(paste("price_sR ~ poly(",variable,",", degree, ")"))
    mod <- lm(formula, data = data)
    res <- predict(mod) - data$price_sR
    mean(res^2)
  })
  test_mse <- sapply(degrees, function(degree) {
    formula <- as.formula(paste("price_sR ~ poly(",variable,",", degree, ")"))
    mod <- lm(formula, data = data)
    res <- predict(mod, newdata = val) - val$price_sR
    mean(res^2)
  })
  
  mse_data <- data.frame(degree = degrees, 
                         train_mse = train_mse, 
                         test_mse = test_mse)
  
  # Combine the MSE data into a data frame
  mse_data <- data.frame(
    degree = degrees,
    train_mse = train_mse,
    test_mse = test_mse
  )
  
  return(mse_data)
}

par(mfrow=c(1,3))

mse_data <- polyy('bath')

irreducible_error <- min(mse_data$test_mse)
train_mse <- mse_data$train_mse
test_mse <- mse_data$test_mse

plot(degrees, train_mse, type = "l", col = "blue", xlab = "Model flexibility", ylab = "Mean squared error",
     main = "bath", ylim = c(min(train_mse)-0.005,max(test_mse)+0.005))
lines(degrees, test_mse, col = "red")
abline(h = irreducible_error, col = "black", lty = "dashed")
legend("topright", legend = c("Training MSE", "Testing MSE", "Irreducible Error"),
       col = c("blue", "red", "black"), lty = c(1, 1, 2))

mse_data <- polyy('rate_sR')

irreducible_error <- min(mse_data$test_mse)
train_mse <- mse_data$train_mse
test_mse <- mse_data$test_mse

plot(degrees, train_mse, type = "l", col = "blue", xlab = "Model flexibility", ylab = "Mean squared error",
     main = "rate_sR", ylim = c(min(train_mse)-0.005,max(test_mse)+0.005))
lines(degrees, test_mse, col = "red")
abline(h = irreducible_error, col = "black", lty = "dashed")
legend("topright", legend = c("Training MSE", "Testing MSE", "Irreducible Error"),
       col = c("blue", "red", "black"), lty = c(1, 1, 2))

mse_data <- polyy('living_sR')

irreducible_error <- min(mse_data$test_mse)
train_mse <- mse_data$train_mse
test_mse <- mse_data$test_mse

plot(degrees, train_mse, type = "l", col = "blue", xlab = "Model flexibility", ylab = "Mean squared error",
     main = "living_sR", ylim = c(min(train_mse)-0.005,max(test_mse)+0.005))
lines(degrees, test_mse, col = "red")
abline(h = irreducible_error, col = "black", lty = "dashed")
legend("topright", legend = c("Training MSE", "Testing MSE", "Irreducible Error"),
       col = c("blue", "red", "black"), lty = c(1, 1, 2))

title(main = 'Figure 5: Training and Test MSEs for Polynomial Models of Varing Covariates', line = -1, outer = TRUE)
```

A series of polynomial regression are ran onto variables included in lasso regression. 
Most variables do not show signs of non-linearity through the MSE changes in varing 
degrees of polynomial regression, with some doing worse at higher flexibilities. 
However, it is worth noting the observations from the variables *bath*, *rate_sR*, *lat* 
and *living_sR* display potential non-linearity (Table 5). Judging by trade-off between high bias 
(left-side) and high variance (right-side), the optimal flexibility for the covariates 
are:

- *bath*: 3, *rate_sR*: 2, *living_sR*: 4

```{r bsp_gam_Fin, cache = TRUE, echo = FALSE}
# returns best gam mod
bosp <- function(data, samples) {
  f <- paste0("price_sR ~ lon + ns(lat) + ns(rate_sR) + hoa + garage + year + ns(bath) + ns(living_sR)")
  f.mse <- 0.5 #baseline (mean/median prediction)
  gam.mod <- NA
  
  for (i in 1:samples) {
    set.seed(i)
    # sample with replacement
    bt_train <- data[sample(nrow(data), replace = TRUE), ]
    
    # gam mod
    bt_mod <- gam::gam(as.formula(f), data = bt_train)
    
    # mse
    mse <- mean((predict(bt_mod,newdata=val)-val$price_sR)^2)
    
    if(mse < f.mse) {
      f.mse <- mse
      gam.mod <- bt_mod
    }
  }
  
  return(gam.mod)
}

gam.mod <- bosp(data,1000)
f.mse <- mean((predict(gam.mod,newdata=val)-val$price_sR)^2)
```
```{r plot_gam_Fin, echo = FALSE, fig.height = 6, fig.width = 12}
par(mfrow=c(2,4))
plot(gam.mod, se = TRUE, col = "red")
title(main="Figure 6: Splines on Covariates Used", line = -1, outer = TRUE)
```
using a bootstrap approach, final testing MSE is `r round(f.mse,4)`

\newpage
\restoregeometry

## Prediction 4: Regression Trees

Decision trees are a different approach to regression. Compared to prior methods, 
decision trees makes decisions based on a hierarchical structure of if-else conditions. 
The nature of decision tree implies high interpretability, quick training times, and 
capable of capturing non-linear relationships.

However, decision trees are prone to overfitting and bias towards features with 
higher levels. To address these issues, various techniques such as **bagging**, 
**random forest** and **boosting** are considered to achieve an optimal tree.

Packages used: **tree**, **randomForest**, **gbm**
```{r tree_setup1_fin,echo = FALSE}
tre <- cv.tree(tree(price_sR ~ ., data = pr), K = 10)
```
```{r tree_setup2_fin, include = FALSE}
png('cv_tree.png',height = 350, width = 480, units = "px")
plot(tre$size, tre$dev, type="b") #its 10
abline(v=10, col="cyan3", lwd=3)

dev.off()

tre <- prune.tree(tree(price_sR ~ ., data = pr), best=10)

pred <- predict(tre,newdata=val)
mse <- mean((pred-val$price_sR)^2)

```

```{r tree_bag_fin, cache = TRUE, include = FALSE}
set.seed(83)

tre.bag <- randomForest(price_sR ~ ., data = pr, mtry = 13, ntree = 200)

pred1 <- predict(tre.bag,newdata=val)
mse1 <- round(mean((pred1-val$price_sR)^2),3)
```
```{r tree_rf_fin, cache = TRUE, include = FALSE}
set.seed(83)

tre.raf <- randomForest(price_sR ~ ., data = pr, mtry = 4, importance = TRUE, ntree = 200)

pred2 <- predict(tre.raf,newdata=val)
mse2 <- round(mean((pred2-val$price_sR)^2),3)
```
```{r tree_bot_fin, cache = TRUE, include = FALSE}
set.seed(83)

tre.bot <- gbm(price_sR ~., data=pr, distribution="gaussian", n.trees=5000, 
               interaction.depth=4)

pred3 <- predict(tre.bot,newdata=val)
mse3 <- round(mean((pred3-val$price_sR)^2),3)
```

```{r interpret_tre_fin, include = FALSE}
png("inte_tre.png",height = 300, width = 480, units = "px")
varImpPlot(tre.bag)
dev.off()
```
\renewcommand{\thefigure}{7}
\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
    \fbox{\includegraphics[width=\linewidth]{cv_tree.png}}
  \caption{Deviance Against Tree Size: Default Tree}
  \bigskip
    \renewcommand{\thefigure}{8}
    \fbox{\includegraphics[width=\linewidth]{inte_tre.png}}
  \caption{Variable Importance: Bagging}
\end{wrapfigure}

Start with cross validation to select best size that balances deviance and overfit. 
this case its 10, prune tree, get results, mse is `r round(mse,3)`.

```{r tree_plot_fin, echo = FALSE}
plot(tre.bag,lwd=2,main='',ylim = c(0.05,0.22))
par(new = TRUE)
plot(tre.raf,lwd=2,col=2,main='',ylim = c(0.05,0.22))

test_error <- NULL
for (i in 1:200) {
  pred <- predict(tre.bot, n.trees = i, newdata = pr)
  test_error <- c(test_error, mean((pred - pr$price_sR)^2))
}

lines(test_error, lwd = 2, col = 3)
legend("topright", legend = c("Bagging", "randomForest: m = sqrt(p)", "Boosting: Depth = 4"),
       col = c(1,2,3),lty = c(2, 2, 2))
title(main = "Figure 9: Test Errors vs No. of Trees: Ensemble Methods", line = -2, outer = TRUE)
```

Applying the ensemble methods, bagging, random forest, and boosting, **the test MSEs**
**are `r mse1`, `r mse2` and `r mse3` respectively.** The performances between the three 
methods are similar and have greatly improved relative to prior methods (MSE-wise). 

The selection of predictors (m) for random forest is set as $\sqrt{p}\approx4$ and 
through off-report tests, a depth of 4 presented the best results for boosting. 
Of the three methods, bagging has achieved the lowest test MSE.

The signifiance of cavoarites determined by the decision trees coincides with 
majority predictions models explored (Figure 8). In particular, similar to GAM, 
the trees also regard *lat* and *lon* as crucial predictors.

Trees in general are highly interpretable, simple, capable of handling complex 
relationships and quick to train at the cost of limited predictive ability. 
Using ensemble methods such as bagging, boosting and random forest, the weaknesses 
in individual trees are mitigated to achieve an improved performance.

\newpage
\restoregeometry

## Prediction 5: Neural Network

Neural networks (NN) are capable of capturing complex nonlinear relationships, making 
them suitable for tasks with intricate data patterns. During training, the network 
adjusts its internal weights and biases to minimize the difference between predicted 
and actual values. Neural network regression can automatically learn relevant 
features from data, but it may be prone to overfitting and requires computational 
resources. 

In this section, NN models are trained using the packages **keras and nnet**. 
A standard NN model is fitted with **nnet** package as as a benchline for further 
adjustments using **Keras**. These adjustments include:

- L1 (Lasso) and L2 (Ridge) regularization: Applies penalty term, to encourage 
smaller models and evenly distributed coefficients. Mitigate the issue of 
multicollinearity and improve the stability and generalization of the model.
- Number of hidden units and layers (HU/HL): Controls complexity of model. The more units 
and layers, the more complex the relationship modeled but triaining times increases 
and may overfit the trainin data.
- Cross-validation splits: The ratio of training data used for model training and 
validation.
- Learning rates (LR): Decides the step size at which the model adjusts its weights. 
Essentially handling how fast or slow the model learns. High rate can cause model 
to coverge to quick while a low learning rate requires more iterations to establish
robust relationships between predictors and responses.
- Dropout rates (DR): Regularization to prevent over-fitting data via dropping nodes 
at random. Thus creates a more robust model that cannot rely on a strong set of 
nodes.
- Callbacks: Options that can be specified to be executed at various points during
the training process. They provide a way to customize and enhance the training 
steps. In this section, early stopping and learning rate adjustments are considered.

Packages: **nnet, tensorflow, caret, keras**

```{r nn_base, results = 'hide'}
nn.base <- nnet(price_sR ~ ., size=25, data = pr, linout = TRUE)
```
```{r nn_base2, include = FALSE}
pred <- predict(nn.base, newdata = val)
mse <- mean((pred-val$price_sR)^2)
```

With *nn.base* model, it contains `r length(nn.base$wts)` weights and measures a 
testing MSE of `r round(mse,3)`.

```{r nn_setup, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE}
set.seed(83)

temp <- as.matrix(data)
temp <- temp[ , !colnames(temp) %in% c("price_sR","id","school")]
dimnames(temp) <- NULL
tempX <- as.matrix(data)
tempX <- tempX[,2]
dimnames(tempX) <- NULL

valX <- as.matrix(val)
valX <- valX[ , !colnames(valX) %in% c("price_sR","id","school")]
dimnames(valX) <- NULL
valY <- as.matrix(val)
valY <- valY[,2]
dimnames(valY) <- NULL

ntestX <- as.matrix(ntest)
ntestX <- ntestX[ , !colnames(ntestX) %in% c("id")]
dimnames(ntestX) <- NULL
```

```{r nn_2Lay, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.first <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  #layer_batch_normalization() %>%
  #layer_dropout(rate = 0.4) %>%
  layer_dense(units = 50, activation = "relu") %>%
  #layer_dropout(rate = 0.3) %>%
  #layer_dense(units = 20, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
  #layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)

# Compile the model
nn.first %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = "mae"
)


hist <- nn.first %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.3)

pred <- nn.first %>% predict(valX)
mse1 <- round(mean((pred-valY)^2),3)

f1 <- nn.first %>% count_params()
```
```{r nnplot_1, include = FALSE}
png("nn_1.png", height = 300, width = 550, units = "px")
plot(hist)
dev.off()
```

```{r nn_reg, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.second <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  #layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 50, activation = "relu", kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.3) %>%
  #layer_dense(units = 20, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
  #layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)

# Compile the model
nn.second %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.002),
  metrics = "mae"
)


hist <- nn.second %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.3)

pred <- nn.second %>% predict(ntestX)

pred <- nn.second %>% predict(valX)
mse2 <- round(mean((pred-valY)^2),3)

f2 <- nn.second %>% count_params()
```
```{r nnplot_2, include = FALSE}
png("nn_2.png", height = 300, width = 550, units = "px")
plot(hist)
dev.off()
```

```{r nn_noBNCB, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.third <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  #layer_batch_normalization() %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 50, activation = "relu", kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 20, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1)

# Compile the model
nn.third %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.002),
  metrics = "mae"
)


hist <- nn.third %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.3)

pred <- nn.third %>% predict(valX)
mse3 <- round(mean((pred-valY)^2),3)

f3 <- nn.third %>% count_params()
```
```{r nnplot_3, include = FALSE}
png("nn_3.png", height = 300, width = 550, units = "px")
plot(hist)
dev.off()
```

```{r nn_fin, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.dropout <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  layer_batch_normalization() %>%
  #layer_dropout(rate = 0.4) %>%
  layer_dense(units = 50, activation = "relu", kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 20, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)

# Compile the model
nn.dropout %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = "mae"
)


hist <- nn.dropout %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.3,
                           callbacks = list(callback_early_stopping(patience = 15), callback_reduce_lr_on_plateau()))

pred <- nn.dropout %>% predict(valX)
mse4 <- round(mean((pred-valY)^2),3)

f4 <- nn.dropout %>%count_params()
```
```{r nnplot_last, include = FALSE}
png("nn_last.png", height = 350, width = 550, units = "px")
plot(hist)
dev.off()
```

The parameters and options are adjusted in 4 models trained with **keras**. The 
parameters, *epochs, batch_size, validation_split, first and second layer units*, 
are fixed at 100, 32, 0.3, 100 and 50 respectively. Furthermore, the rectified 
linear unit (ReLU) activation function, $g(z)$, is applied during the training 
of all models.


\begin{table}[h]
    \caption{NN Summary Statistics}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|c|c|c|c}
     \textbf{Model} & \textbf{Parameters} & \textbf{Specifications} & \textbf{Test MSE}\\
      \hline
      base & `r length(nn.base$wts)` & 1 HL, 25 HU & `r round(mse,3)`\\
      nnmod.(a) & `r f1` & 2 HL, (100,50) HU, LR = 0.005 & `r mse1`\\
      nnmod.(b) & `r f2` & 2 HL, (100,50) HU, LR = 0.002, DR = (0.4,0.3), $\lambda_{Ridge}$ = 0.001 & `r mse2`\\
      nnmod.(c) & `r f3` & 3 HL, (100,50,20) HU, LR = 0.002, DR = (0.3,0.2,0.1), $\lambda_{lasso}$ = $\lambda_{Ridge}$ =  0.001 & `r mse3`\\
      nnmod.(d) & `r f4` & LR = 0.005, DR = (batch normalization,0.2,0.1), $\lambda_{lasso}$ = $\lambda_{Ridge}$ = 0.001, callback = Early stopping, Reduce LR on plateau & `r mse3`\\
    \end{tabular}%
    }
\end{table}


The NN models (Table 7) share similar testing MSEs ranging between 0.14 to 0.17. 
All NN models trained through keras has improved from the base NN model based on 
the testing MSEs but plateus at 0.145 for the final two models with most tuning.

Model (a), with a high learning rate, highly overfits the training data as training 
loss is lower than validation loss (Fig.9), thus it is not reliable on unseen data. 
**The best performing NN model is nnmod.(b)** with 2 hidden layers (100 units to 50 units
to 1 unit), a learning rate of 0.002, drop rate of 0.4 to 0.3 and $\lambda_{Ridge}$ 
= 0.001. The increase in MSE in models (c) and (d) may be subject to overfitting. 
With more parameters than available training data (n = 4000), more parameters and layers 
might have exacerbate the issue of limited data, and cause struggle in generalization 
of final weights. Additionally, the model may start to fit the noise rather than 
the underlying patterns, resulting in a higher MSE when applied to unseen data.

\renewcommand{\thefigure}{9}
\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nn_1.png}
  \caption[Network2]%
  {{\small 2 layers: 100->50->1}}   
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}  
  \centering 
  \includegraphics[width=\textwidth]{nn_2.png}
  \caption[]%
  {{\small + Ridge \& Dropout}}    
\end{subfigure}
\vskip\baselineskip
\begin{subfigure}[b]{0.4\textwidth}   
  \centering 
  \includegraphics[width=\textwidth]{nn_3.png}
  \caption[]%
  {{\small 3 layers:100->50->20->1+Lasso}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}   
  \centering 
  \includegraphics[width=\textwidth]{nn_last.png}
  \caption[]%
  {{\small + Batch Norm \& Callbacks}}
\end{subfigure}
\caption[ The average and standard deviation of critical parameters ]
{\small MSE (loss) and MAE of keras Models} 
\end{figure*}

## Prediction: Discussion and Conclusion

\begin{wraptable}{r}{8cm}
    \caption{Summary of Prediction Models}
    \begin{tabular}{c|c}
     \textbf{Model} & \textbf{Testing MSE}\\
      \hline
      Naive: Median & 0.568\\
      Naive: Mean & 0.472\\
      Linear: Best Subset Selection (CV) & 0.248\\
      Linear: Lasso Regularization (CV) & 0.244\\
      Polynomial and Splines (Bootstrap) & 0.243\\
      Trees: Base (CV) & 0.224\\
      Trees: Random forest & 0.095\\
      Trees: Boosting & 0.095\\
      \textbf{Trees: Bagging} & \textbf{0.091}\\
      NN: Base (nnet) & 0.169\\
      NN: Best (keras) & 0.140\\
      NN: Over-paramterized (keras) & 0.145\\
    \end{tabular}
\end{wraptable}

This section we review the prediction models trained to predict the response, 
*price_sR* (Table 8). For each method and class, the trained models demonstrate progressive 
improvement in testing MSE with the tree model trained through a Bagging approach 
achieving the best MSE of 0.091.

Notably, the respective coefficients, significance, and weights for predictors *lat* 
and *lon* are more crucial in polynomial and spline models, tree models and NN models. 
Suggesting a non-linear relationship between *lat*, *lon* and *price_sR*. Further 
investigation in underlying relationship are needed to achieve better results. 

Furthermore, in tree models and NN models, *bath* is not addressed as significant 
as linear and polynomial models. This raises the concern of multicollinearity between 
*bath* and *living_sR* (Table 2) in our models.

In the prediction models, *living_sR* is significant in predicting *price_sR*. 
Which was anticipated as the most correlated covariate (Table 2).

Apart from multicollinearity, two other major limitations restricts model performance. 
Firstly, missing data was addressed with multiple imputation which only partially recovers 
the bias and explanatory power of covariates. Secondly, given the ratio of 
5:3 between known and unknown data, the models trained may not accurately capture 
the full complexity and variability of the underlying patterns and may struggle to 
generalize well to unseen data. These issues needs to be properly addressed in 
following analysis to train more robust models.

\newpage

## Naive Classificaitons

```{r clas_setup_Fin, echo = FALSE, results = 'hide'}
cl <- data[which(!colnames(data) %in% c("id",'price_sR'))]
```

```{r nc_baseline_FIN, include = FALSE, cache = TRUE}
n <- nrow(cl)
K <- 10
acc <- rep(NA,K)
acc2 <- rep(NA,K)

fold <- sample(rep(1:K, each = n/K))

for(k in 1:K) {
  
  temp <- cl
  temp$school <- as.factor(temp$school)
  df.train <- temp[fold != k,]
  df.test <- temp[fold == k,]
  
  temp <- naive_bayes(school ~ ., data = df.train)
  pred <- predict(temp, newdata = df.test)

  acc[k] <- sum(pred == df.test$school) / nrow(df.test)
  
  df.train$school = as.numeric(df.train$school)
  df.test$school = as.numeric(df.test$school)
  df.trainX <- df.train[,-1]
  df.trainY <- df.train[,1]
  df.testX <- df.test[,-1]
  df.testY <- df.test[,1]
  
  pred <- knn(train = df.trainX, test = df.testX, cl = df.trainY, k = 5) ##check appendix, 5 is this best
  acc2[k] <- sum(as.numeric(pred) == df.testY) / length(df.testY)
}
```
There are a few simple classification techniques to predict whether a school is 
nearby the housing (the covariate *school*). Naive Bayes classifier and K-Nearest-Neighbours 
(KNN) are naive classification techniques that make simple assumptions about the data. 
Using classification rate (CR) as performance criterion, a 10-fold 
cross validation is conducted and the average correct classification rate is used 
as benchmark for further analysis.

Naive Bayes classifier is a probabilistic classifier while KNN is a non-parametric 
classifier that assigns categories to an observation based on majority vote of its 
*k* nearest neighbours.

Through packages *naivebayes* and *class*, the CRs are `r round(mean(acc),3)` and 
`r round(mean(acc2),3)` for Naive Bayes and 5-KNN.


## Classification: Logsitic Model

Logistic model is a type of generalized linear model (GLM) that models the probability 
of a binary class. It maps a linear combination of predictors to a value between 0 
and 1 using a form of $\frac{exp(p(x))}{1+exp(p(x)}$.

A backward selection method is used to identify the most influential variables. 
Then a boostrap is conducted to retrieve the best performing coefficients using a
default threshold of 0.5 and cauchit link function. 

Packages used: *stats*
```{r lm_fs, eval=FALSE}
mod <- glm(school ~ ., data = cl, family = binomial(link = 'cauchit'))
mod <- step(mod, direction = "backward")
```
```{r lm_fs2, include=FALSE}
mod <- glm(school ~ ., data = cl, family = binomial(link = 'cauchit'))
mod <- step(mod, direction = "backward")
```
```{r lm_fs3, echo = FALSE}
print(formula(mod))
```

```{r lm_btfn, cache=TRUE, echo = FALSE}
f <- formula(mod)

acc_bs <- rep(0,1000)

# Perform B bootstrap replicates
for (i in 1:1000) {
  set.seed(i)
  # Sample with replacement from the original data
  bt_train <- cl[sample(nrow(cl), replace = TRUE), ]
  
  # Fit the model to the bootstrapped data
  bt_mod <- glm(f, data = bt_train, family = binomial(link = 'cauchit'))
  
  pred <- predict(bt_mod, newdata = val, type = 'response')
  pred <- ifelse(pred > 0.5, 1, 0)
  
  confusion <- table(pred,val$school)
  
  bt_acc <- (confusion[1,1]+confusion[2,2])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])
  
  acc_bs[i] <- bt_acc
  
  if(max(acc_bs) == bt_acc) {
    f.lm.mod <- bt_mod
  }
}

```

```{r lm_bt1, cache = TRUE, echo = FALSE}
f.lm.mod <- bosp(cl, 1000, 0.5, 'cauchit')
```

```{r lm_bt2, cache = TRUE, include=FALSE}
pred <- predict(f.lm.mod, newdata = val, type = 'response')

threshold <- seq(0.01, 0.8, by=0.01)
n.t <- length(threshold)
overall <- rep(0, n.t)
fp <- rep(0, n.t)
fn <- rep(0, n.t)

for(i in 1:n.t){
  pred_lo <- ifelse(pred > i/100, 1, 0)
  confusion <- table(pred_lo,val$school)
  
  #fp
  t1 <- tryCatch({
    confusion['0',]
  }, error = function(err) {
    c(0, 0)
  })
  
  #fn
  t2 <- tryCatch({
    confusion['1',]
  }, error = function(err) {
    c(0, 0)
  })
  
  overall[i] <- (t1[2]+t2[1])/(sum(t1)+sum(t2))
  fp[i] <- ifelse(t1[2]==0, 0, t1[2]/(sum(t1)))
  fn[i] <-ifelse(t2[1]==0, 0, t2[1]/(sum(t2)))
}

png("lm_thres1.png", height = 300, width = 500)
plot(threshold,overall,type='l',lwd=3,ylim=c(0,0.8), ylab="error rate")
lines(threshold, fp, col="orange", lwd=3)
lines(threshold, fn, col="blue", lwd=3)
legend("topright", legend=c("overall", "false.pos", "false.neg"), col=c("black", "orange", "blue"),
lty=c(1,1,1), lwd=3 )
dev.off()
```

```{r lm_roc, include = FALSE}
png("lm_roc.png", height = 300, width = 500)
par(mfrow=c(1,2))

pred_lo <- ifelse(pred > 0.5, 1, 0)
plot(roc(val$school,pred_lo), main = "Threshold: 0.5", xlab = "False Positive Rate", ylab = "True Positive Rate", print.auc = TRUE)

pred_lo <- ifelse(pred > 0.6, 1, 0)
plot(roc(val$school,pred_lo), main = "Threshold: 0.6", xlab = "False Positive Rate", ylab = "True Positive Rate", print.auc = TRUE)

dev.off()

pred_lo <- ifelse(pred > 0.5, 1, 0)
confusion <- table(pred_lo,val$school)
t1 <- confusion['0',]
t2 <- confusion['1',]
```

\renewcommand{\thefigure}{10}
\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
    \fbox{\includegraphics[width=\linewidth]{lm_thres1.png}}
  \caption{CR in Varing Thresholds}
  \bigskip
    \renewcommand{\thefigure}{11}
    \fbox{\includegraphics[width=\linewidth]{lm_roc.png}}
  \caption{ROC Curves}
\end{wrapfigure}

The error rate (overall) is minimized with a threshold of 0.6 (33.0%) compared to 
the default assumption of 0.5 (34.1%) (Figure 10). However, using a threshold of 
0.5 results in a higher AUC (0.657) than 0.6 (0.598) (Figure 11). While the overall 
classification rate provides a general measure of prediction performance, AUC-ROC 
provides a more comprehensive evaluation on the prediction by also considering 
trade-off between FP and FN. Therefore, using a threshold of 0.5 is appropriate for 
this prediction.

The model suggests housing with more bedrooms, higher tax rates are less likely 
to have a school nearby. A potential reason may be that housing with higher tax rates and more 
bedrooms are located at a more wealthy neighborhood with no schools nearby. 

**The final logistic model achieves an accuracy rate of 0.679**, with approriate false-positive (FP) 
rate of 0.276 and false-negative (FN) rate of 0.399. The significance and implications 
of variables selected are discussed alongside discriminant analysis in the next section.

\newpage
\restoregeometry

## Classification: Discriminant Analysis

While logistic regression is utlised for classifying binary or categorical variables, 
discriminant analysis can be used when the response variable is categorical and 
represents distinct groups. There are also differences in assumptions, discriminant 
analysis emphasizes on independence, equal variance and normality of covariates. 

A k-fold-cross-validation forward selection approach is used to identify the most 
influential variables. The subset of predictors are passed onto model training with 
linear and quadratic discriminant analysis.

```{R lda_setup, include = FALSE, cache = TRUE}
n <- nrow(cl)
K <- 5
set.seed(824)
fold <- sample(rep(1:K, each=n/K))

best_mean_err_ld <- 1

unused_cov <- colnames(cl)
unused_cov <- unused_cov[which(!unused_cov == 'school')]
used_cov <- NULL

covs <- colnames(cl)

switch <- TRUE

while(switch) {
  switch <- FALSE
  
  if(length(unused_cov) == 0) {
    break
  }
  
  add_cov <- NULL
  
  for(cov in unused_cov) {
    errs <- rep(NA,K)
    
    cur_cov <- c(used_cov, cov)
    f <- paste0("school~",paste(cur_cov,collapse="+"))
    
    for(k in 1:K) {
      df.train <- cl[fold != k,]
      df.test <- cl[fold == k,]
      
      p0 <- sum(df.train$school == 0) / nrow(df.train)
      p1 <- sum(df.train$school == 1) / nrow(df.train)
      
      temp_mod <- lda(as.formula(f), data = df.train, prior = c(p0, p1))
      
      lda_pred <- predict(temp_mod, newdata = df.test)$class
      actual <- df.test$school
      
      # confusion matrix
      confusion <- table(lda_pred,actual)
      
      # error rate [Miss classification]
      errs[k] <- (confusion[1,2]+confusion[2,1])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])
    }
    
    mean_err <- mean(errs)
    
    if(mean_err < best_mean_err_ld) {
      print(paste0(cov,' ',mean_err))
      best_mean_err_ld <- mean_err
      ld_sd <- sqrt(sum((errs - mean_err)^2) / (K - 1))
      add_cov <- cov
      switch <- TRUE
    }
  }
  
  used_cov <- c(used_cov,add_cov)
  unused_cov <- unused_cov[unused_cov!=add_cov]
}
```

```{r lda_finalMod, echo = FALSE}
p0 <- sum(val$school == 0) / nrow(val)
p1 <- sum(val$school == 1) / nrow(val)


f1 <- paste0("school~",paste(used_cov,collapse="+"))
temp_mod <- lda(as.formula(f1), data = val, prior = c(p0, p1))

lda_pred <- predict(temp_mod, newdata = val)$class
actual <- val$school

# confusion matrix
confusion_lda <- table(lda_pred,actual)

# error rate [Miss classification]
err_lda <- (confusion[1,2]+confusion[2,1])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])
```

```{R qda_setup, include = FALSE}
n <- nrow(cl)
K <- 10
set.seed(824)
fold <- sample(rep(1:K, each=n/K))

best_mean_err_qd <- 1

unused_cov <- colnames(cl)
unused_cov <- unused_cov[which(!unused_cov == 'school')]
used_cov <- NULL

covs <- colnames(cl)

switch <- TRUE

while(switch) {
  switch <- FALSE
  
  if(length(unused_cov) == 0) {
    break
  }
  
  add_cov <- NULL
  
  for(cov in unused_cov) {
    errs <- rep(NA,K)
    
    cur_cov <- c(used_cov, cov)
    f <- paste0("school~",paste(cur_cov,collapse="+"))
    
    for(k in 1:K) {
      df.train <- cl[fold != k,]
      df.test <- cl[fold == k,]
      
      p0 <- sum(df.train$school == 0) / nrow(df.train)
      p1 <- sum(df.train$school == 1) / nrow(df.train)
      
      temp_modX <- qda(as.formula(f), data = df.train, prior = c(p0, p1))
      
      qda_pred <- predict(temp_modX, newdata = df.test)$class
      actual <- df.test$school
      
      # confusion matrix
      confusion <- table(qda_pred,actual)
      
      # error rate [Miss classification]
      errs[k] <- (confusion[1,2]+confusion[2,1])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])
    }
    
    mean_err <- mean(errs)
    
    if(mean_err <= best_mean_err_qd) {
      print(paste0(cov,' ',mean_err))
      best_mean_err_qd <- mean_err
      qd_sd <- sqrt(sum((errs - mean_err)^2) / (K - 1))
      add_cov <- cov
      switch <- TRUE
    }
  }
  
  used_cov <- c(used_cov,add_cov)
  unused_cov <- unused_cov[unused_cov!=add_cov]
}
```

```{r qda_finalMod, echo = FALSE}
p0 <- sum(val$school == 0) / nrow(val)
p1 <- sum(val$school == 1) / nrow(val)


f2 <- paste0("school~",paste(used_cov,collapse="+"))
temp_modX <- qda(as.formula(f2), data = val, prior = c(p0, p1))

qda_pred <- predict(temp_modX, newdata = val)$class
actual <- val$school

# confusion matrix
confusion_qda <- table(qda_pred,actual)

# error rate [Miss classification]
err_qda <- (confusion[1,2]+confusion[2,1])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])
```

```{r lm_ffplot, include = FALSE}
png("conf_lq.png", height = 300, width = 500)
par(mfrow=c(1,2))
ctable <- as.table(matrix(c(confusion_lda[1,1], confusion_lda[1,2], confusion_lda[2,1], confusion_lda[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Method: LDA")

ctable <- as.table(matrix(c(confusion_qda[1,1], confusion_qda[1,2], confusion_qda[2,1], confusion_qda[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Method: QDA")
dev.off()
```

<!--
```{r lq_roc, include = FALSE}
png("lq_roc.png", height = 300, width = 500)
par(mfrow=c(1,2))

plot(roc(val$school,as.numeric(lda_pred)-1), main = "Method: LDA", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

plot(roc(val$school,as.numeric(qda_pred)-1), main = "Method: QDA", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

dev.off()
```
-->

```{r decision_bound, include = FALSE, message = FALSE, warning = FALSE}
x1_grid <- seq(min(data$lat), max(data$lat), length.out = 100)
x2_grid <- seq(min(data$lon), max(data$lon), length.out = 100)
grid <- data.frame(cbind(x1_grid,x2_grid))
colnames(grid) <- c('lat','lon')

predictions <- predict(temp_mod, newdata = grid)
class_probabilities <- predictions$posterior

s1 <- temp_mod$scaling[1]
s2 <- temp_mod$scaling[2]
ld <- as.vector(temp_mod$means[1, ]) %*% temp_mod$scaling
linear_discriminant <- as.vector(grid$lat) * s2 + as.vector(grid$lon) * s1 + ld
px <- predict(temp_mod, newdata = grid, type = "var")$x[, 1]

confidence_intervals <- 1.96 * sqrt(sum((px-mean(px))^2)/99)/10
lower_bound <- linear_discriminant - confidence_intervals
upper_bound <- linear_discriminant + confidence_intervals


png("db.png", height = 300, width = 500)

plot(data$lat, data$lon, col = ifelse(data$school==1, 3,4), pch = 19, xlab = "lat", ylab = "lon")
lines(x1_grid, linear_discriminant, col = "red", lwd = 2)
lines(x1_grid, lower_bound, col = 1, lty = 2)
lines(x1_grid, upper_bound, col = 1, lty = 2)

legend("topright", legend = c("School Nearby","School Not Nearby"),
       col = c(3,4), lty = c(1, 1))

dev.off()
```

\renewcommand{\thefigure}{12}
\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
    \fbox{\includegraphics[width=\linewidth]{db.png}}
  \caption{Linear Decision Boundary and Confidence Intervals}
  \bigskip
    \renewcommand{\thefigure}{13}
    \fbox{\includegraphics[width=\linewidth]{conf_lq.png}}
  \caption{Confusion Matrix on Validation Set}
\end{wrapfigure}

### Linear Discriminant Analysis (LDA)

`r f1`

The latitude (*lat*) and longtitude (*lon*) are sole covariates chosen by LDA (5-fold). 
Apart from the intuitive implication that the geographic coordinates are 
determinative of whether a school is nearby (*school*) (Figure 12.), it also implies that the 
structure of housing does not significant change the posterior probabilities on 
the classification of *school* linearly.

**The LDA model achieves an accuracy rate of 0.695** (Figure 13) alongside an AUC-ROC score of 
0.638.

### Quadratic Discriminant Analysis (QDA)

`r f2`

Similarly, QDA (10-fold) selects longtitude (*lon*) and latitude (*lat*) as predictors 
for *school*. Additionally, QDA also chooses presence of heating/cooling/ventilation
system (*hvac*), Home Owner Association (*hoa*), garage spaces (*garage*) and lot 
size (*lot_sR*) as covariates influential for the classification of *school*. 

Notably, the group means of *hoa* between the decision boundaries suggest housing 
without a school nearby tend to not have a HOA.

**The QDA model achieves an accuracy rate of 0.733** (Figure 13) alongside an AUC-ROC score of 
0.721 (Figure 13).

### GLM, LDA and QDA

```{r CIs, include = FALSE}
ld_d <- best_mean_err_ld - 1.96 * ld_sd / 2.236
ld_u <- best_mean_err_ld + 1.96 * ld_sd / 2.236

qd_d <- best_mean_err_qd - 1.96 * qd_sd / 3.162
qd_u <- best_mean_err_qd + 1.96 * qd_sd / 3.162

rates <- 1-acc_bs
lm_sd <- sqrt(sum((rates - mean(rates))^2) / 999)
lm_d <- mean(rates) - 1.96 * lm_sd / 31.622
lm_u <- mean(rates) + 1.96 * lm_sd / 31.622
```

\begin{wraptable}{r}{5cm}
    \caption{Confidence Intervals}
    \begin{tabular}{c|c}
     \textbf{Model} & \textbf{CI}\\
      \hline
      GLM (Bootstrap) & (`r round(lm_d,3)`,`r round(lm_u,3)`)\\
      LDA (5-fold CV) & (`r round(ld_d,3)`,`r round(ld_u,3)`)\\
      QDA (10-fold CV) & (`r round(qd_d,3)`,`r round(qd_u,3)`)\\
    \end{tabular}
\end{wraptable}

Inspecting the predictors of GLM, LDA, and QDA, there is a similarity of using 
*lat* and *lon* as covariates for response variable *school*, thus reinforces 
the importance of the two predictors. Contrast to GLM, through forward selection, 
LDA and QDA trains a simpler model with classification rates within their 
respective 95% confidence intervals (Table 10).

Overall, **LDA and QDA models performs slightly better on the validation set** relative 
to GLM (Table 10). For further studies, it should be noted that LDA and QDA 
generally performs better with larger datasets to estimate covariance matrices 
more accurately. Whilst not a major issue in this training dataset and predictor 
subset, discriminant analysis are also more sensitive to multicollinearity relative to GLM.

\newpage
\restoregeometry

## Classification: Classification Decision trees

Similar to regression trees, classification decision trees makes decisions based 
on a hierarchical structure of if-else conditions. The difference between the two 
models include:

- Different criterion: In a classification setting, variance measures such as RSS 
cannot be used. Alternative measures are error (mis-classification) rates, Gini index 
and cross-entropy.
- Data partitioning: Classification trees aim to partition the data into groups 
as accurate as possible using branches of condition statements. While regression 
trees capture and creating groups to contain similar responses.

Once again, decision trees are prone to overfitting and bias towards features with 
higher levels. Therefore, various techniques such as **bagging**, **random forest** 
and **boosting** are considered to achieve an optimal tree to address the issues.

Packages used: **tree**, **randomForest**, **gbm**

```{r ctree_setup1_fin,results = 'hide'}
tre <- cv.tree(tree(as.factor(school) ~ ., data = cl), K = 10)
```
```{r ctree_setup2_fin, include = FALSE}
png('cv_clas_tree.png',height = 350, width = 480, units = "px")
plot(tre$size, tre$dev, type="b") #its 11
abline(v=7, col="cyan3", lwd=3)

dev.off()

tre <- prune.tree(tree(as.factor(school) ~ ., data = cl), best=11)

pred <- predict(tre,newdata=val, type = 'class')
pred <- as.numeric(pred)-1
actual <- val$school
confusion <- table(pred,actual)
      
acc <- (confusion[1,1]+confusion[2,2])/1000

```

```{r cl_tree_bag_fin, cache = TRUE, include = FALSE}
set.seed(83)

tre.bag <- randomForest(as.factor(school) ~ ., data = cl, mtry = 13, ntree = 200)

pred1 <- predict(tre.bag,newdata=val, type = 'class')
confusion1 <- table(pred1,actual)
acc1 <- (confusion1[1,1]+confusion1[2,2])/1000
```
```{r cl_tree_rf_fin, cache = TRUE, include = FALSE}
set.seed(83)
class_labels <- levels(data$Class)

tre.raf <- randomForest(as.factor(school) ~ ., data = cl, mtry = 4, importance = TRUE, ntree = 200)

pred2 <- predict(tre.raf,newdata=val, type = 'class')
confusion2 <- table(pred2,actual)
acc2 <- (confusion2[1,1]+confusion2[2,2])/1000
```
```{r cl_tree_bot_fin, cache = TRUE, include = FALSE}
set.seed(83)

tre.bot <- gbm(school ~., data=cl, distribution="bernoulli", n.trees=5000, 
               interaction.depth=4)

pred3 <- predict(tre.bot,newdata=val, type = 'response')
pred3 <- ifelse(pred3 > 0.5, 1, 0)
confusion3 <- table(pred3,actual)
acc3 <- (confusion3[1,1]+confusion3[2,2])/1000
```

```{r interpret_cl_tre_fin, include = FALSE}
png("cv_inte_tre.png",height = 300, width = 480, units = "px")
plot(tre)
text(tre, pretty = 0)
dev.off()
```
\renewcommand{\thefigure}{14}
\begin{wrapfigure}{r}{0.4\textwidth}
  \centering
    \fbox{\includegraphics[width=\linewidth]{cv_tree.png}}
  \caption{Deviance and Against Clssification Tree Size}
  \bigskip
    \renewcommand{\thefigure}{15}
    \fbox{\includegraphics[width=\linewidth]{cv_inte_tre.png}}
  \caption{Pruned Classification Tree - 11}
\end{wrapfigure}

Start with cross validation to select best size that balances deviance and overfit. 
this case its 11, prune tree, get results, mse is `r acc`.

```{r cl_tree_plot_fin, echo = FALSE, warning = FALSE, message = FALSE}
par(mar=c(5,2,2,5),mfrow = c(2,3))
ctable <- as.table(matrix(c(confusion1[1,1], confusion1[1,2], confusion1[2,1], confusion1[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1)

ctable <- as.table(matrix(c(confusion2[1,1], confusion2[1,2], confusion2[2,1], confusion2[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1)

ctable <- as.table(matrix(c(confusion3[1,1], confusion3[1,2], confusion3[2,1], confusion3[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1)

plot(roc(val$school,as.numeric(pred1)-1), main = "Method: Bagging", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

plot(roc(val$school,as.numeric(pred2)-1), main = "Method: Random Forest", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

plot(roc(val$school,as.numeric(pred3)-1), main = "Method: Boosting", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

title(main = "Figure 16: Summary for Classification Trees", line = -1, outer = TRUE)
```

Applying the ensemble methods, bagging, random forest, and boosting, **the test MSEs**
**are `r acc1`, `r acc2` and `r acc3` respectively** (Figure 16). Correspondingly, 
the methods achieves AUC scores of *0.958*, *0.942*, and *0.932*. The performances 
between the three methods are similar and have improved significantly relative 
to prior methods (Classification rate and ROC-AUC). 

The selection of predictors (m) for random forest is set as $\sqrt{p}\approx4$ and 
through off-report tests, a depth of 4 presented the best results for boosting. 
Of the three methods, bagging has achieved the lowest test MSE.

\newpage
\restoregeometry

## Classification: Support Vector Machines (SVM)

In prior sections, decision trees significantly outperform logistics and discriminant 
regression methods. Decision trees excel in capturing relationships without explicit 
prior feature selection and are more robust to outliers. Which conjugates that 
the data may be distinctivly partitioned unique groups. If the conjecture is true, 
than support vector machines (SVM) should also outperform logstics and discriminant 
methods since SVM aims to find a plane to separate groups of classifications.

Package used: *e1071*

```{r svm_base}
svm.mod <- svm(as.factor(school) ~ ., data = cl, kernel = 'linear', cost = 10, scale = FALSE)
```
```{r svm_base_results, echo = FALSE}
temp1 <- val
temp1$school <- as.factor(temp1$school)

ypred <- predict(svm.mod, temp1)
tab <- table(predict=ypred, truth=temp1$school)
```
As a benchmark model, we fit a linear SVM with a cost argument of 10. This benchmark 
model achieves an accuracy of `r sum(diag(tab))/1000` and AUC value of 0.639. 
To fine-tune the SVM model, the cost and kernel parameters are optimized using 
*tune()*. Which utilizes cross-validation to find combinations of parameter values 
that result in the best performance. The kernal functions examined differs as such:

- Linear: Trains a linear decision boundary. From prior models, this kernal should 
have the lowest accuracy as data do not share an apparent linear relationship with 
*school*. 
- Radial Basis: Trains a hyperplane decision boundary. By mapping the data to a 
higher dimensional plane, this method aims to distinguish linear and non-linear 
relationships between the covariates and response variables.
- Polynomial: Trains a non-linear decision boundary. It is able to capture high 
complexity relationships but can be prone to overfit if not tuned correctly.

<!-- this part takes long to run give don't change -->
```{r svm_tune, cache = TRUE, echo = FALSE}
lin_svm <- tune(svm, as.factor(school) ~ ., data=cl, kernel ="linear",
     ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100) ))

rad_svm <- tune(svm, as.factor(school) ~ ., data=cl, kernel ="radial",
     ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100) ))

pol_svm <- tune(svm, as.factor(school) ~ ., data=cl, kernel ="polynomial",
     ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100) ))
```

```{r svm_confusion, echo = FALSE}
temp <- lin_svm$best.model

pred1 <- predict(temp, temp1)
confusion1 <- table(predict=pred1, truth=temp1$school)

temp <- rad_svm$best.model

pred2 <- predict(temp, temp1)
confusion2 <- table(predict=pred2, truth=temp1$school)

temp <- pol_svm$best.model

pred3 <- predict(temp, temp1)
confusion3 <- table(predict=pred3, truth=temp1$school)
```

```{r svm_plot_fin, include = FALSE, warning = FALSE, message = FALSE}
png('smv_sum.png', height = 600, width = 1000, units = "px")

par(mar=c(5,2,2,5),mfrow = c(2,3))
ctable <- as.table(matrix(c(confusion1[1,1], confusion1[1,2], confusion1[2,1], confusion1[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1)

ctable <- as.table(matrix(c(confusion2[1,1], confusion2[1,2], confusion2[2,1], confusion2[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1)

ctable <- as.table(matrix(c(confusion3[1,1], confusion3[1,2], confusion3[2,1], confusion3[2,2]), nrow = 2, byrow = TRUE,
                          dimnames = list(c("0","1"),c("0","1"))))
names(dimnames(ctable)) <- c("Predicted","Actual")
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1)

plot(roc(val$school,as.numeric(pred1)-1), main = "Function: Linear", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

plot(roc(val$school,as.numeric(pred2)-1), main = "Function: Radial", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

plot(roc(val$school,as.numeric(pred3)-1), main = "Function: Polynomial", xlab = "False Positive Rate", 
     ylab = "True Positive Rate", print.auc = TRUE)

dev.off()
```

\renewcommand{\thefigure}{17}
\begin{wrapfigure}{r}{12cm}
  \centering
    \fbox{\includegraphics[width=12cm]{smv_sum.png}}
  \caption{Summary for SVM}
\end{wrapfigure}

Summary of tuning (Figure 17):

- Linear
  + Accuracy: 0.662
  + AUC: 0.618
  + Cost: 0.001

- Radial
  + Accuracy: 0.843
  + AUC: 0.825
  + Cost: 100

- Polynomial
  + Accuracy: 0.800
  + AUC: 0.779
  + Cost: 100

Overall, radial kernal presents the most optimal prediction results on the validation 
data set through CV with polynomial kernals' performance similar. Notably, linear 
kernal is least effective as conjugeted, reinforcing the hypothesis that covariates 
share a non-linear relationship with *school* (Figure 12, 17).

\newpage
\restoregeometry

## Classification: Neural Network

Neural networks (NN) are also capable of performing classification tasks. Similar 
to using NN for prediction of *price_sR*, NN models are trained using packages 
**keras and nnet**. The differences underlying classification NN and prediction NN 
include,

- Difference in evaluation methods: Cost functions used to estimate model performance 
and describe discrpency between estimated and actual values. For classification models 
in this section a binary cross-entropy loss function is used.
- Difference in performance metrics: Loss (MSE) and mean absolute error rate (MAE) 
are common criterions used for regression. Classification utilities accuracy, precision, 
and F1 score to assess performance of the model. For classification models in this 
section accuracy is used.
- Activation functions: Classification employs activation functions that provides 
categorical or binary outputs such as sigmoid (binary) and softmax (multiclass).

The adjustments to parameters are similar to prediction with NN in prior section.

Packages: **nnet, tensorflow, caret, keras**

```{r cnn_base, results = 'hide'}
nn.base <- nnet(as.factor(school) ~ ., size=25, data = cl, decay=5e-4, maxit=200)
```
```{r cnn_base2, include = FALSE}
pred <- predict(nn.base, newdata = val)
pred <- ifelse(pred > 0.5, 1, 0)

actual <- val$school
confusion <- table(pred,actual)

```

With benchmark NN model, it contains `r length(nn.base$wts)` weights and measures a 
testing accuracy of `r sum(diag(confusion))/1000`.

```{r cnn_setup, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE}
set.seed(83)

temp <- as.matrix(data)
temp <- temp[ , !colnames(temp) %in% c("price_sR","id","school")]
dimnames(temp) <- NULL
tempX <- as.matrix(data)
tempX <- tempX[,3]
dimnames(tempX) <- NULL

valX <- as.matrix(val)
valX <- valX[ , !colnames(valX) %in% c("price_sR","id","school")]
dimnames(valX) <- NULL
valY <- as.matrix(val)
valY <- valY[,3]
dimnames(valY) <- NULL

ntestX <- as.matrix(ntest)
ntestX <- ntestX[ , !colnames(ntestX) %in% c("id")]
dimnames(ntestX) <- NULL
```

```{r cnn_2Lay, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.first <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  #layer_batch_normalization() %>%
  #layer_dropout(rate = 0.4) %>%
  layer_dense(units = 50, activation = "relu") %>%
  #layer_dropout(rate = 0.3) %>%
  #layer_dense(units = 20, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
  #layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compile the model
nn.first %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.002),
  metrics = "accuracy"
)


hist <- nn.first %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.3)

pred <- nn.first %>% predict(valX)
pred <- ifelse(pred > 0.5, 1, 0)
confusion1 <- table(pred,valY)

f1 <- nn.first %>% count_params()
```
```{r cnnplot_1, include = FALSE}
png("cnn_1.png", height = 300, width = 550, units = "px")
plot(hist)
dev.off()
```

```{r cnn_reg, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.second <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  #layer_batch_normalization() %>%
  #layer_dropout(rate = 0.3) %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  #layer_dense(units = 20, activation = "relu", kernel_regularizer = regularizer_l1(0.001)) %>%
  #layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compile the model
nn.second %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = "accuracy"
)


hist <- nn.second %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.2)

#pred <- nn.second %>% predict(ntestX)

pred <- nn.second %>% predict(valX)
pred <- ifelse(pred > 0.5, 1, 0)
confusion2 <- table(pred,valY)

f2 <- nn.second %>% count_params()
```
```{r cnnplot_2, include = FALSE}
png("cnn_2.png", height = 300, width = 550, units = "px")
plot(hist)
dev.off()
```

```{r cnn_noBNCB, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.third <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  #layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 25, activation = "relu") %>%
  layer_dropout(rate = 0.05) %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compile the model
nn.third %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.003),
  metrics = "accuracy"
)


hist <- nn.third %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.2)


pred <- nn.third %>% predict(valX)
pred <- ifelse(pred > 0.5, 1, 0)
confusion3 <- table(pred,valY)

f3 <- nn.third %>% count_params()
```
```{r cnnplot_3, include = FALSE}
png("cnn_3.png", height = 300, width = 550, units = "px")
plot(hist)
dev.off()
```

```{r cnn_fin, cache = TRUE, echo = FALSE, messages = FALSE, warning = FALSE, fig.show='hold', result = 'hide'}
set.seed(83)
nn.dropout <- keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = c(13)) %>%
  layer_batch_normalization() %>%
  #layer_dropout(rate = 0.4) %>%
  layer_dense(units = 50, activation = "relu", kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 25, activation = "relu") %>%
  layer_dropout(rate = 0.05) %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compile the model
nn.dropout %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = "accuracy"
)


hist <- nn.dropout %>% fit(temp, tempX, epochs = 100, batch_size = 32, validation_split = 0.3,
                           callbacks = list(callback_early_stopping(patience = 10), callback_reduce_lr_on_plateau()))


pred <- nn.dropout %>% predict(valX)
pred <- ifelse(pred > 0.5, 1, 0)
confusion4 <- table(pred,valY)

f4 <- nn.dropout %>%count_params()
```
```{r cnnplot_last, include = FALSE}
png("cnn_last.png", height = 350, width = 550, units = "px")
plot(hist)
dev.off()
```

The parameters and options are adjusted in 4 models trained with **keras**. The 
parameters, *epochs, batch_size, validation_split, first and second layer units*, 
are fixed at 100, 32, 0.3, 100 and 50 respectively. Furthermore, the rectified 
linear unit (ReLU) activation function, $g(z)$, is applied during the training 
of all models.


\begin{table}[h]
    \caption{NN Summary Statistics}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|c|c|c|c}
     \textbf{Model} & \textbf{Parameters} & \textbf{Specifications} & \textbf{Test Accuracy}\\
      \hline
      base & `r length(nn.base$wts)` & 1 HL, 25 HU & `r sum(diag(confusion))/1000`\\
      nnmod.(a) & `r f1` & 2 HL, (100,50) HU, LR = 0.002 & `r sum(diag(confusion1))/1000`\\
      nnmod.(b) & `r f2` & 2 HL, (100,50) HU, LR = 0.005, DR = (0,0.1) & `r sum(diag(confusion2))/1000`\\
      nnmod.(c) & `r f3` & 3 HL, (100,50,20) HU, LR = 0.003, DR = (0.2,0.1,0.05) & `r sum(diag(confusion3))/1000`\\
      nnmod.(d) & `r f4` & LR = 0.005, DR = (batch normalization,0.1,0.05), $\lambda_{lasso}$ = 0.001, callback = Early stopping, Reduce LR on plateau & `r sum(diag(confusion4))/1000`\\
    \end{tabular}%
    }
\end{table}


The NN models (Table 10) share similar classification rates ranging between 0.83 to 0.86. 
**The best performing NN model is nnmod.(b)** with 2 hidden layers (100 units to 50 units
to 1 unit) and a learning rate of 0.002. Importantly, the utilization of regularization 
and dropout techniques did not produce improved outcomes. This can be attributed 
to various factors, such as insufficient data, persistent underfitting of the model, 
or inadequate parameter tuning. This aspect will be considered a caveat for this project.

Similarly, the decrease in accuracy of models (c) and (d) is attributable to 
underfit and limited by data size. With more parameters than available training 
data (n = 4000), excessive parameters and layers exacerbates the issue of limited data, 
which results difficulty generalizing final weight values. Additionally, the model 
may start to fit the noise rather than the underlying patterns, resulting in a 
decrease in accuracy when applied to unseen data.

\renewcommand{\thefigure}{18}
\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
  \centering
  \includegraphics[width=\textwidth]{cnn_1.png}
  \caption[Network2]%
  {{\small 2 layers: 50->25->1}}   
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}  
  \centering 
  \includegraphics[width=\textwidth]{cnn_2.png}
  \caption[]%
  {{\small + Ridge \& Dropout}}    
\end{subfigure}
\vskip\baselineskip
\begin{subfigure}[b]{0.4\textwidth}   
  \centering 
  \includegraphics[width=\textwidth]{cnn_3.png}
  \caption[]%
  {{\small 3 layers:100->50->20->1+Lasso}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}   
  \centering 
  \includegraphics[width=\textwidth]{cnn_last.png}
  \caption[]%
  {{\small + Batch Norm \& Callbacks}}
\end{subfigure}
\caption[ The average and standard deviation of critical parameters ]
{\small Binary Cross-Entropy (loss) and Accuracy of keras Models} 
\end{figure*}


\newpage
## Classification: Discussion and Conclusion:

\begin{wraptable}{r}{8cm}
    \caption{Summary of Classification Models}
    \begin{tabular}{c|c}
     \textbf{Model} & \textbf{Testing Accuracy}\\
      \hline
      Naive Bayes & 0.671\\
      KNN (5K) & 0.772\\
      Logistic: Backward Selection (Bootstrap) & 0.679\\
      LDA: Forward Selection (5CV) & 0.695\\
      QDA: Forward Selection (10CV) & 0.733\\
      Trees: Base (10CV) &  0.778\\
      Trees: Random forest & 0.949\\
      Trees: Boosting & 0.939\\
      \textbf{Trees: Bagging} & \textbf{ 0.961}\\
      SVM: Base (Linear) & 0.658\\
      SVM: Linear (Tuned) & 0.662\\
      SVM: Radial (Tuned) & 0.843\\
      SVM: Polynomial (Tuned) & 0.800\\
      NN: Base (nnet) & 0.834\\
      NN: Best (keras) & 0.863\\
      NN: Over-paramterized (keras) & 0.832\\
    \end{tabular}
\end{wraptable}

This section we review the classification models trained to predict the response, 
*school* (Table 11). The best performing approaches are decisions trees, SVM and 
nerual networks with a bagging tree model achieving the highest accuracy of 0.961 on 
validation data.

Notably, the covariates *lat* and *lon* are regarded as significant predictors 
in majority models. The significance becomes more pronounced in models examining 
non-linear relationships between variables, indicating non-linear relationship between 
*lat*, *lon* and *school* (Figure 12). Further investigation in underlying relationship is 
warranted to train better models.

While performing the analysis, two major limitations restricting model performance 
is discovered. Firstly, missing data, preprocessed with multiple imputation, failed to
recover the lost bias and thus the analysis excluded the patterns in missing data. 
Secondly, given the unbalanced ratio etween known and unknown data, the models 
fitted have limited effectiveness in capturing the full complexity and variability 
of the underlying patterns. These issues needs to be properly addressed in 
following analysis to train more robust models.

\newpage
\restoregeometry

## Appendix

```{r knn_setup, include = FALSE}
k <- 1:20


n <- nrow(cl)
K <- 10
acc <- rep(NA,K)

fold <- sample(rep(1:K, each = n/K))
test.er <- rep(0, length(k))
for(c in 1:20){
  acc2 <- rep(NA,K)
  for(k in 1:K) {
    
    temp <- cl
    temp$school <- as.factor(temp$school)
    df.train <- temp[fold != k,]
    df.test <- temp[fold == k,]
    
    temp <- naive_bayes(school ~ ., data = df.train)
    pred <- predict(temp, newdata = df.test)
    
    acc[k] <- sum(pred == df.test$school) / nrow(df.test)
    
    df.train$school = as.numeric(df.train$school)
    df.test$school = as.numeric(df.test$school)
    df.trainX <- df.train[,-1]
    df.trainY <- df.train[,1]
    df.testX <- df.test[,-1]
    df.testY <- df.test[,1]
    
    knn.pred <- knn(train = df.trainX, test = df.testX, cl = df.trainY, k = c)
    
    
    pred <- knn(train = df.trainX, test = df.testX, cl = df.trainY, k = 15)
    acc2[k] <- sum(as.numeric(pred) == df.testY) / length(df.testY)
  }
  
  test.er[c] <- mean(acc2[k])
}
```

```{r knn_plot, echo = FALSE, out.height='60%', out.width='60%',fig.align='center'}
plot(1:20,test.er,lwd=2,ylab = 'Test Acc',xlab = 'X Nearest Neighbours', type = 'l', main = 'Test Accuracy and K-Nearest-Neighbours')
```

## References

1. Enders, C. K. (2022). *Applied Missing Data Analysis.* Guilford Publications.
2. Woods, A. D., Gerasimova, D., Van Dusen, B., Nissen, J. M., Bainter, S. A., Uzdavines, A., Davis-Kean, P. E., Halvorson, M. A., King, K. M., Logan, J. a. R., Xu, M., Vasilev, M. R., Clay, J. M., Moreau, D., Joyal-Desmarais, K., Cruz, R. A., Brown, D. M. Y., Schmidt, K., & Elsherif, M. (2023). Best practices for addressing missing data through multiple imputation. Infant and Child Development. https://doi.org/10.1002/icd.2407
3. Griffin, M. M., & Steinbrecher, T. D. (2013). Large-Scale Datasets in Special Education Research. In *Elsevier eBooks* (pp. 155–183). https://doi.org/10.1016/b978-0-12-407760-7.00004-9
4. James, G., Witten, D., Trevor, H., Tibshirani, R. (2021). *An Introduction to Statistical Learning* Springer

## R-Packages
1. Baptiste Auguie (2017). gridExtra: Miscellaneous Functions for "Grid" Graphics. R package version 2.3. https://CRAN.R-project.org/package=gridExtra
2. JJ Allaire and François Chollet (2023). keras: R Interface to 'Keras'. R package version 2.11.1. https://CRAN.R-project.org/package=keras
3. JJ Allaire and Yuan Tang (2022). tensorflow: R Interface to 'TensorFlow'. R package version 2.11.0. https://CRAN.R-project.org/package=tensorflow
4. Barret Schloerke, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg and Jason Crowley (2021). GGally: Extension to 'ggplot2'. R package version 2.1.2. https://CRAN.R-project.org/package=GGally
5. Brandon Greenwell, Bradley Boehmke, Jay Cunningham and GBM Developers (2022). gbm: Generalized Boosted Regression Models. R package version 2.1.8.1. https://CRAN.R-project.org/package=gbm
6. David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel and Friedrich Leisch (2021). e1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. R package version 1.7-9. https://CRAN.R-project.org/package=e1071
7. H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.
8. Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2022). dplyr: A Grammar of Data Manipulation. R package version 1.0.8. https://CRAN.R-project.org/package=dplyr
9. Jarek Tuszynski (2021). caTools: Tools: Moving Window Statistics, GIF, Base64, ROC AUC, etc. R package version 1.18.2. https://CRAN.R-project.org/package=caTools
10. John Fox and Sanford Weisberg (2019). An {R} Companion to Applied Regression, Third Edition. Thousand Oaks CA: Sage. URL: https://socialsciences.mcmaster.ca/jfox/Books/Companion/
11. Kuhn, M. (2008). Building Predictive Models in R Using the caret Package. Journal of Statistical Software, 28(5), 1–26. https://doi.org/10.18637/jss.v028.i05
12. Majka M (2019). naivebayes: High Performance Implementation of the Naive Bayes Algorithm in R. R package version 0.9.7, <URL: https://CRAN.R-project.org/package=naivebayes>.
13. Revelle, W. (2021) psych: Procedures for Personality and Psychological Research, Northwestern University, Evanston, Illinois, USA, https://CRAN.R-project.org/package=psych Version = 2.1.9.
14. R Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
15. Stef van Buuren, Karin Groothuis-Oudshoorn (2011). mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software, 45(3), 1-67. DOI 10.18637/jss.v045.i03.
16. Taiyun Wei and Viliam Simko (2021). R package 'corrplot': Visualization of a Correlation Matrix (Version 0.92). Available from https://github.com/taiyun/corrplot
17. Thomas Lumley based on Fortran code by Alan Miller (2020). leaps: Regression Subset Selection. R package version 3.1. https://CRAN.R-project.org/package=leaps
18. Tierney N, Cook D (2023). “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” Journal of Statistical Software, 105(7), 1-31. doi: 10.18637/jss.v105.i07 (URL: https://doi.org/10.18637/jss.v105.i07).
19. Trevor Hastie (2023). gam: Generalized Additive Models. R package version 1.22-2. https://CRAN.R-project.org/package=gam
20. Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0
21. Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0
22. Xavier Robin, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez and Markus Müller (2011). pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics, 12, p. 77. DOI: 10.1186/1471-2105-12-77 http://www.biomedcentral.com/1471-2105/12/77/
23. Yihui Xie (2021). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.37.
24. Wood, S.N. (2011) Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the Royal Statistical Society (B) 73(1):3-36
